<html>
<head>
  <meta charset="UTF-8">
  <meta name=viewport content="width=710">
  <meta name="description" content="Personal blog about machine learning">
  <meta name="keywords" content="< DOPUNI ">
  <meta name="author" content="mcerovic">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"> </script>
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,500" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../../style.css">
  <link rel="shortcut icon" type="image/png" href="../../favicon.png"/>
  <script>
    <!-- GA -->
  </script>
	<script> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, CommonHTML: {scale: 100}});</script>
  <title> Logistic Regression </title>
</head>
<body>
<div>
  <!-- title -->
	<p>
		<span>m</span>cerovic
		<span> blog about machine learning algorithms </span>
	</p>
  <!-- navigation -->
  <div>
    <ul>
      <li><a href="../../index.html"> Notes </a></li> &middot;
      <li><a href="../../projects.html"> Projects </a></li> &middot;
      <li><a href="../../topics.html"> Topics </a></li> &middot;
      <li><a href="../../archive.html"> Archive </a></li> &middot;
      <li><a href="../../about.html"> About </a></li>
    </ul>
  </div>
  <!-- post container -->
	<div>
		<div class="note">
			<a href=""> Logistic Regression </a>
			<span> Supervised Learning, Classification, Discriminative Learning, Generalized Linear Models </span>
      <!-- contents -->
      <ul class="contents">
        <li> <a href="#logistic_regression"> 1. Logistic regression </a> </li>
        <li> <a href="#regularized_logistic_regression"> 2. Regularized logistic regression </a> </li>
        <li> <a href="#"> 3. Resources </a> </li>
      </ul>
      <div>
        In preparation...
        <!-- <h1 id="logistic_regression">1. Logistic regression </h1>
  			<p>

          Logistic regression is an another supervised learning algorithm from the GLM family. Event though it's name suggests regression (continual value) we are talking about binary <i> classification </i> algorithm.

          The classification problem can be e.g. determing if a bank transaction is a fraud or not, is an email spam or not, what language given text is and so on.

          For a given data set $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}) \mid \mathbf{x}^{(i)} \in \mathbb{R}^{n}, y^{(i)} \in \{0, 1\} \}_{i = 1}^{m}$ we have to determine to which class does some new instance $\mathbf{x}^{(k)}$, $\mathbf{x}^{(k)} \notin \mathcal{D}$, belongs.

          We said that all algorithms from GLM family are derived using some probability distribution. <a href="../LinearRegression/index.html"> Linear regression </a> is choosing Gaussian distribution with variable mean that best describe the data. In this case we have to choose Bernoulli distribution for discribing binary data, so $ y^{(i)} \mathop{\sim}\limits^{{\small\text{i.i.d.}}} \mathcal{Bernoulli}(h_{\mathbf{w}}(\mathbf{x}^{(i)})) $. When we determined distribution that will help us find loss function function.

          The main idea behind the logistic regression is to predict the probability $ p(y = 1 | \mathbf{x}; \mathbf{w}) = h_{\mathbf{w}}(\mathbf{x}) $ that instance belongs to class $1$ and the probability of that instance to belong to another class is $ p(y = 0 | \mathbf{x};\mathbf{w}) = 1 - p(y = 1 | \mathbf{x};\mathbf{w})$.

          Next step is to find logical hypothesis. Linear model $ \mathbf{w}^{T} \mathbf{x} $ has a codomain in interval $[-\infty, \infty]$ and we have to find a differentiabile and monotholicy growing function which maps $[-\infty, \infty] \to [0, 1]$.

          There are functions which meet these conditions and one of them is the sigmoid or logistic function :

          $$ f(z) = \frac{1}{1 + e^{-z}} $$

          To see if this is working we can check out the limits : $ \lim_{z \rightarrow -\infty} f(z) = 0$ and $ \lim_{z \rightarrow \infty} f(z) = 1$.

          Another reasone for choosing sigmoid function is because it has a simple derivative :

          $$ \frac{d}{dz}f(z) =  f(z)(1 - f(z)) $$

          <img class="himg" width="400px" src="https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" alt="Logistic curve" />

          So by combining linear model $ \mathbf{w}^{T} \mathbf{x} $ and sigmoid function we get the hypothesis :

          $$ h_{\mathbf{w}}(\mathbf{x})  = f(\mathbf{w}^{T} \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^{T} \mathbf{x} }} $$

          Now let's derive the loss function for the logistic regression. If we combine probabalistic interpretations for the classes we have :

          $$ p(y|\mathbf{x};\mathbf{w}) = (h_{\mathbf{w}}(\mathbf{x}))^{y} (1 - h_{\mathbf{w}}(\mathbf{x}))^{1 - y} $$

          And to proof this, if the class label is $y = 0, \; p(y|\mathbf{x};\mathbf{w}) = 1 - h_{\mathbf{w}}(\mathbf{x})$ and if $ y = 1, \; p(y|\mathbf{x};\mathbf{w}) =h_{\mathbf{w}}(\mathbf{x})$.

          As same as for the linear regression we want to estimate the parameters $ \mathbf{w} $ so that hypothesis $\hat{h}_{\mathbf{w}}$ best fit the data.

          The empirical risk minimization principle states that we shoud choose hypothesis $\hat{h}_{\mathbf{w}}$ which minimizes the empirical risk $ R_{emp}(h_{\mathbf{w}}) $.

          If we choose for the loss function negative log likelihood function $ L(y^{(i)}, \hat{y}) = - \log p(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{w}) $ the ERM will be the same as MLE and then the hypothesis we should choose is :

          $$
              \eqalign {
                  \hat{h}_{\mathbf{w}} &= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} R_{emp} (h_{\mathbf{w}})
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} \log p(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{w})
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} \log (h_{\mathbf{w}}(\mathbf{x}))^{y} (1 - h_{\mathbf{w}}(\mathbf{x}))^{1 - y}
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))
              }
          $$

          Not let's derive the derivative for the logistic regression :

          $$
            \eqalign {
                &\frac{\partial }{\partial w_{j}} R_{emp}(h_{\mathbf{w}}) =
                \cr&= \frac{\partial }{\partial w_{j}} \Bigg( - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Bigg)
                \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \frac{\partial }{\partial w_{j}} \Bigg( y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Bigg)
                \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Bigg( y^{(i)} \frac{1}{h_{\mathbf{w}}(\mathbf{x})} + (1 - y^{(i)}) \frac{1}{1 - h_{\mathbf{w}}(\mathbf{x})} \Bigg) \frac{\partial }{\partial w_{j}} h_{\mathbf{w}}(\mathbf{x})
                \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Bigg( y^{(i)} \frac{1}{h_{\mathbf{w}}(\mathbf{x})} + (1 - y^{(i)}) \frac{1}{1 - h_{\mathbf{w}}(\mathbf{x})} \Bigg) f(\mathbf{w}^{T} \mathbf{x})(1 - f(\mathbf{w}^{T} \mathbf{x})) \frac{\partial }{\partial w_{j}} \mathbf{w}^{T} \mathbf{x}
                \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Big( y^{(i)}(1 - h_{\mathbf{w}}(\mathbf{x})) + (1 - y^{(i)})h_{\mathbf{w}}(\mathbf{x}) \Big) x_{j}^{(i)}
                \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Big( y^{(i)} - h_{\mathbf{w}}(\mathbf{x}) \Big) x_{j}^{(i)}
            }

          $$

        </p>

        <h1 id="regularized_logistic_regression"> 2. Regularized logistic regression </h1>
        <p>
          Regularized form of the logistic regression with $\ell_{2}$ regularization term is :

          $$
            \hat{h}_{\mathbf{w}} = \arg\min_{h_{\mathbf{w}} \in \mathcal{H}} \bigg\{
              - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) + \lambda \sum_{j = 1}^{n} w_{j}^{2}
            \bigg\}
          $$

        </p>

        <h1 id="resources"> 3. Resources </h1>
        <p>
          <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf"> CS229: Supervised Learning, Discriminative Algorithms. - Andrew Ng </a> <br>
  				<a href="https://www.microsoft.com/en-us/research/people/cmbishop/"> Pattern Recognition and Machine Learning. - Christopher Bishop </a> <br>
        </p> -->

		  </div>
	  </div>
  </div>
	<p> Copyright &copy; 2016 </p>
</div>
</body>
</html>
