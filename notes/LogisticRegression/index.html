<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta name=viewport content="width=710">
  <meta name="description" content="Logistic regression is an another supervised learning algorithm from the GLM family. Event though its name suggests predicting continual value we are talking about binary classification algorithm...">
  <meta name="keywords" content="Machine learning, Supervised learning, Discriminative models, Generalized Linear Models, Classification, Logistic Regression, Maximum Likelihood Estimation For Logistic Regression, Regularized Logistic Regression, Empirical Risk Mimization For Logistic Regression">
  <meta name="author" content="mcerovic">
  <link rel="canonical" href="https://mcerovic.github.io/notes/LogisticRegression/"/>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"> </script>
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,500" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../../style.css">
  <link rel="shortcut icon" type="image/png" href="../../favicon.png"/>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-86497875-1', 'auto');
    ga('send', 'pageview');
  </script>
	<script> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, CommonHTML: {scale: 100}});</script>
  <title> Logistic Regression </title>
</head>
<body>
<div>
  <!-- title -->
	<p>
		<span>m</span>cerovic
		<span> blog about machine learning algorithms </span>
	</p>
  <!-- navigation -->
  <div>
    <ul>
      <li><a href="../../index.html"> Notes </a></li> &middot;
      <li><a href="../../projects.html"> Projects </a></li> &middot;
      <li><a href="../../topics.html"> Topics </a></li> &middot;
      <li><a href="../../archive.html"> Archive </a></li> &middot;
      <li><a href="../../about.html"> About </a></li>
    </ul>
  </div>
  <!-- post container -->
	<div>
		<div class="note">
			<a href=""> Logistic Regression </a>
			<span> Supervised Learning, Classification, Discriminative Learning, Generalized Linear Models </span>
      <!-- contents -->
      <ul class="contents">
        <li> <a href="#logistic_regression"> 1. Logistic regression </a> </li>
        <li> <a href="#regularized_logistic_regression"> 2. Regularized logistic regression </a> </li>
        <li> <a href="#resources"> 3. Resources </a> </li>
      </ul>
      <div>

        <h1 id="logistic_regression">1. Logistic regression </h1>
  			<p>

          Logistic regression is an another supervised learning algorithm from the GLM family. Event though its name suggests predicting continual value we are talking about binary <i> classification </i> algorithm.

          The classification problem can be e.g. determining if a bank transaction is a fraud or not, if an email is spam or not, what language the given text is in and so on.

          For a given supervised data set $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}) \mid \mathbf{x}^{(i)} \in \mathbb{R}^{n+1}, y^{(i)} \in \{0, 1\} \}_{i = 1}^{m}$ we have to determine to which class does some new instance $\mathbf{x}^{(k)}$, $\mathbf{x}^{(k)} \notin \mathcal{D}$, belongs.

          We said that all algorithms from the GLM family are derived from some probability distribution. <a href="../LinearRegression/index.html"> Linear regression </a> is choosing Gaussian distribution with the variable mean that best describes the data. In this case we have to choose Bernoulli distribution for discribing binary data, so $ y^{(i)} \mathop{\sim}\limits^{{\small\text{i.i.d.}}} \mathcal{Bernoulli}(h_{\mathbf{w}}(\mathbf{x}^{(i)})) $.

          The main idea behind the logistic regression is to predict the probability $ p(y = 1 | \mathbf{x}; \mathbf{w}) = h_{\mathbf{w}}(\mathbf{x}) $ that an instance belongs to class $1$ and the probability of that instance to belong to another class is $ p(y = 0 | \mathbf{x};\mathbf{w}) = 1 - h_{\mathbf{w}}(\mathbf{x})$.

          The next step is to find logical hypothesis. Linear model $ \mathbf{w}^{T} \mathbf{x} $ has a codomain in interval $[-\infty, \infty]$ and we have to find a differentiabile and monotholicy growing function which maps $[-\infty, \infty] \to [0, 1]$.

          There are functions which meet these conditions and one of them is the <i> sigmoid </i> or <i> logistic </i> function :

          $$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

          To see if this is working we can check out the limits : $ \lim_{z \rightarrow -\infty} \sigma(z) = 0$ and $ \lim_{z \rightarrow \infty} \sigma(z) = 1$.

          Another reason for choosing sigmoid function is because it has a simple derivative :

          $$ \frac{d}{dz}\sigma(z) =  \sigma(z)(1 - \sigma(z)) $$

          So by combining the linear model $ \mathbf{w}^{T} \mathbf{x} $ and sigmoid function we get the hypothesis :

          $$ h_{\mathbf{w}}(\mathbf{x})  = \sigma(\mathbf{w}^{T} \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^{T} \mathbf{x} }} $$

          Plot of the sigmoid function :

          <img class="himg" width="400px" src="https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" alt="Logistic curve" />
          <span style="margin-top: -15px; font-size: 15px; display:block; text-align: center;"> Wikipedia figure</span>

          The linear model $ \mathbf{w}^{T} \mathbf{x} $ is a hyperplane and from the plot we can conclude that if $ \mathbf{w}^{T} \mathbf{x} \geq 0 $ the probability of $ p(y = 1 | \mathbf{x}; \mathbf{w}) \geq 0.5 $ for class $ 1 $ and if $ \mathbf{w}^{T} \mathbf{x} \lt 0 $ the probability of $ p(y = 0 | \mathbf{x}; \mathbf{w}) \lt 0.5 $ for class $ 0 $. By combining these two our classifier will predict :

          $$
            h_{\mathbf{w}}(\mathbf{x}) =
              \begin{cases}
                1,  & \text{if $\sigma(\mathbf{w}^{T} \mathbf{x}) \geq 0.5 $ } \\[1ex]
                0,  & \text{if $\sigma(\mathbf{w}^{T} \mathbf{x}) \lt 0.5 $}
            \end{cases}
          $$

          Now let's derive the loss function for the logistic regression. If we combine probabalistic interpretations for the classes we have :

          $$ p(y|\mathbf{x};\mathbf{w}) = (h_{\mathbf{w}}(\mathbf{x}))^{y} (1 - h_{\mathbf{w}}(\mathbf{x}))^{1 - y} $$

          And to prove this, if the class label is $y = 0, \; p(y|\mathbf{x};\mathbf{w}) = 1 - h_{\mathbf{w}}(\mathbf{x})$ and if $ y = 1, \; p(y|\mathbf{x};\mathbf{w}) =h_{\mathbf{w}}(\mathbf{x})$.

          As in the case of the linear regression we want to estimate the parameters $ \mathbf{w} $ so that hypothesis $\hat{h}_{\mathbf{w}}$ best fit the data.

          The empirical risk minimization principle states that we shoud choose hypothesis $\hat{h}_{\mathbf{w}}$ which minimizes the empirical risk $ R_{emp}(h_{\mathbf{w}}) $.

          If we choose for the loss function negative log likelihood function $ L(y^{(i)}, \hat{y}) = - \log p(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{w}) $ empirical risk minimization will be the same as maximum likelihood estimation and then the hypothesis we should choose is :

          $$
              \eqalign {
                  \hat{h}_{\mathbf{w}} &= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} R_{emp} (h_{\mathbf{w}})
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} \log p(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{w})
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} \log (h_{\mathbf{w}}(\mathbf{x}))^{y} (1 - h_{\mathbf{w}}(\mathbf{x}))^{1 - y}
                                    \cr&= \arg \min_{h_{\mathbf{w}} \in \mathcal{H}} - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))
              }
          $$

          The above optimization problem can be solved via <a href="../GradientDescent/index.html"> gradient descent </a> and a derivative for the logistic regression is :

          $$
            \frac{\partial }{\partial w_{j}} R_{emp}(h_{\mathbf{w}}) = - \frac{1}{m} \sum_{i = 1}^{m} \Big( y^{(i)} - h_{\mathbf{w}}(\mathbf{x}^{(i)}) \Big) x_{j}^{(i)}
          $$

          <!--
          \frac{\partial }{\partial w_{j}} \Bigg( - \frac{1}{m} \sum_{i = 1}^{m} y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) \\ &+ (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Bigg)
          \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \frac{\partial }{\partial w_{j}} \Bigg( y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Bigg)
          \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Bigg( y^{(i)} \frac{1}{h_{\mathbf{w}}(\mathbf{x})} + (1 - y^{(i)}) \frac{1}{1 - h_{\mathbf{w}}(\mathbf{x})} \Bigg) \frac{\partial }{\partial w_{j}} h_{\mathbf{w}}(\mathbf{x})
          \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Bigg( y^{(i)} \frac{1}{f(\mathbf{w}^{T} \mathbf{x})} + (1 - y^{(i)}) \frac{1}{1 - f(\mathbf{w}^{T} \mathbf{x})} \Bigg) f(\mathbf{w}^{T} \mathbf{x})(1 - f(\mathbf{w}^{T} \mathbf{x})) \frac{\partial }{\partial w_{j}} \mathbf{w}^{T} \mathbf{x}
          \cr&= - \frac{1}{m} \sum_{i = 1}^{m} \Big( y^{(i)}(1 - h_{\mathbf{w}}(\mathbf{x})) + (1 - y^{(i)})h_{\mathbf{w}}(\mathbf{x}) \Big) x_{j}^{(i)} -->

        </p>

        <h1 id="regularized_logistic_regression"> 2. Regularized logistic regression </h1>
        <p>
          Regularized form of logistic regression with $\ell_{2}$ regularization term is :

          $$
            \begin{align}
              \hat{h}_{\mathbf{w}} = \arg\min_{h_{\mathbf{w}} \in \mathcal{H}} \bigg\{&
                - \frac{1}{m} \sum_{i = 1}^{m} \big( y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))
                \\ &+ (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \big) + \lambda \sum_{j = 1}^{n} w_{j}^{2}
              \bigg\}
            \end{align}
          $$

        </p>

        <h1 id="resources"> 3. Resources </h1>
        <p>
          <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf"> CS229: Supervised Learning, Discriminative Algorithms. - Andrew Ng </a> <br>
  				<a href="https://www.microsoft.com/en-us/research/people/cmbishop/"> Pattern Recognition and Machine Learning. - Christopher Bishop </a> <br>
        </p>

		  </div>
	  </div>
  </div>
	<p> Copyright &copy; 2016 </p>
</div>
</body>
</html>
